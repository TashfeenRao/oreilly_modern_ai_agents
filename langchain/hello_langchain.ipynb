{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dda531c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d775710",
   "metadata": {},
   "source": [
    "### Setup / Grabbing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de522bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded docs and stored in chroma vector store\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://ai-office-hours.beehiiv.com/p/beyond-benchmarks\",\n",
    "    \"https://ai-office-hours.beehiiv.com/p/evaluating-ai-agent-tool-selection\",\n",
    "    \"https://ai-office-hours.beehiiv.com/p/re-ranking-rag\",\n",
    "    \"https://ai-office-hours.beehiiv.com/p/quantizing-llms-llama-3\",\n",
    "    \"https://ai-office-hours.beehiiv.com/p/llm-probing\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [doc for sublist in docs for doc in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    collection_name=\"rag-chroma\",\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "print(\"loaded docs and stored in chroma vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b650b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top-line conversation starter when evaluating an L https://ai-office-hours.beehiiv.com/p/beyond-benchmarks\n",
      "(like testing a model’s financial tool selecting a https://ai-office-hours.beehiiv.com/p/beyond-benchmarks\n",
      "Beyond BenchmarksAI Office HoursLoginSubscribe0AI  https://ai-office-hours.beehiiv.com/p/beyond-benchmarks\n",
      "fine-tuning process. Test sets in general (benchma https://ai-office-hours.beehiiv.com/p/beyond-benchmarks\n"
     ]
    }
   ],
   "source": [
    "question = \"tell me about benchmarks\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:50], doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ac078",
   "metadata": {},
   "source": [
    "### Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "954a51d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary score: yes\n",
      "page content: top-line conversation starter when evaluating an L\n",
      "\n",
      "Document Source: https://ai-office-hours.beehiiv.com/p/beyond-benchmarks\n",
      "binary score: yes\n",
      "page content: (like testing a model’s financial tool selecting a\n",
      "\n",
      "Document Source: https://ai-office-hours.beehiiv.com/p/beyond-benchmarks\n",
      "binary score: yes\n",
      "page content: Beyond BenchmarksAI Office HoursLoginSubscribe0AI \n",
      "\n",
      "Document Source: https://ai-office-hours.beehiiv.com/p/beyond-benchmarks\n",
      "binary score: yes\n",
      "page content: fine-tuning process. Test sets in general (benchma\n",
      "\n",
      "Document Source: https://ai-office-hours.beehiiv.com/p/beyond-benchmarks\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(description=\"A binary score of yes or no indicating if the document answers the question.\")\n",
    "    \n",
    "    \n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"Give a binary score of 'yes' or 'no' indicating if the document answers the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"user\", \"Question: {question}\\n\\n Document: {document}\\n\")\n",
    "])\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "for doc in docs:\n",
    "    result = retrieval_grader.invoke({\n",
    "        \"question\": question,\n",
    "        \"document\": doc.page_content\n",
    "    })\n",
    "    print(f\"binary score: {result.binary_score}\")\n",
    "    print(f\"page content: {doc.page_content[:50]}\\n\")\n",
    "    print(f\"Document Source: {doc.metadata['source']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b96e31",
   "metadata": {},
   "source": [
    "### Generate Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "600ab9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.SystemMessagePromptTemplate'>\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "----\n",
      "<class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
      "Question: {question}\n",
      "\n",
      "Context: {context}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext: {context}\")\n",
    "])\n",
    "\n",
    "for message in prompt.messages:\n",
    "    print(type(message))\n",
    "    print(message.prompt.template)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d183f101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "Benchmarks are standardized open-source test sets used to evaluate AI models on specific tasks, ensuring fair comparison by using agreed-upon train/validation/test splits. They serve as a useful starting point to shortlist models but should not be the sole measure of performance, as models can exploit biases or shortcuts in benchmarks, leading to inflated scores without true understanding. Additionally, over-optimizing for benchmarks can cause models to \"game\" the system, and high benchmark scores do not necessarily indicate real-world effectiveness or generalization.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(\"Generated Answer:\")\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cd619",
   "metadata": {},
   "source": [
    "### Question Re-write / The corrective Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ba5788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Question:\n",
      "What are benchmarks and how are they used to measure performance in different fields?\n"
     ]
    }
   ],
   "source": [
    "bigger_llm = ChatOpenAI(model_name=\"gpt-4.1\", temperature=0.1)\n",
    "\n",
    "system = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "re_writer_chain = re_write_prompt | bigger_llm | StrOutputParser()\n",
    "improved_question = re_writer_chain.invoke({\"question\": question})\n",
    "print(\"Improved Question:\")\n",
    "print(improved_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
